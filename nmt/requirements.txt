# Requirements for Gemma LoRA Fine-tuning
torch>=2.0.0
transformers>=4.35.0
peft>=0.6.0
accelerate>=0.24.0
datasets>=2.14.0
bitsandbytes>=0.41.0
scipy>=1.10.0
wandb>=0.15.0
trl>=0.7.0

# Optional for better performance
flash-attn>=2.3.0
ninja
packaging

# For evaluation
sacrebleu
rouge-score
nltk